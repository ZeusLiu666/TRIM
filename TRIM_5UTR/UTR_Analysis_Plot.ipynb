{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a281288e",
   "metadata": {},
   "source": [
    "# Generating temp files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2002cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def merge_and_enrich_trim_data(\n",
    "    trim_file_path=\"data/TRIM_Ded1_Dataset.csv\", \n",
    "    pars_file_path=\"data/denome_wide_PARS_dataset.csv\", \n",
    "    output_file_path=\"data/TRIM_Ded1_Dataset_Enhanced.csv\"\n",
    "):\n",
    "    \"\"\"\n",
    "    1. Load TRIM and PARS Dataset.\n",
    "    2. Merge 'Max30Pos' column in PARS Dataest with TRIM Dataset(Based on alignment of Gene and #YORF).\n",
    "    3. Add 'split' column and 'SystematicName' column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load CSV file\n",
    "    try:\n",
    "        df_trim = pd.read_csv(trim_file_path)\n",
    "        df_pars = pd.read_csv(pars_file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return\n",
    "\n",
    "    df_merged = pd.merge(\n",
    "        df_trim, \n",
    "        df_pars[['#YORF', 'Max30Pos']], \n",
    "        left_on='tx_id', \n",
    "        right_on='#YORF', \n",
    "        how='left'\n",
    "    )\n",
    "    df_merged.drop(columns=['#YORF'], inplace=True)\n",
    "    df_merged['split'] = 'test'\n",
    "    df_merged['SystematicName'] = df_merged['Gene']\n",
    "    cols = list(df_merged.columns)\n",
    "    df_merged.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"File saved as {output_file_path}\")\n",
    "    print(f\"Raw data lines: {len(df_trim)}\")\n",
    "    print(f\"Merged data lines: {len(df_merged)}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_and_enrich_trim_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff651b",
   "metadata": {},
   "source": [
    "# Calculating Pairing Probability\n",
    "**Caution: Switch RNA2d mamba env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35973a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def predict_structure(sequence):\n",
    "    \"\"\"\n",
    "    Predicting RNA secondary structure\n",
    "    \"\"\"\n",
    "    try:\n",
    "        process = subprocess.Popen(\n",
    "            ['RNAfold', '--noPS'], \n",
    "            stdin=subprocess.PIPE,\n",
    "            stdout=subprocess.PIPE,\n",
    "            stderr=subprocess.PIPE,\n",
    "            text=True\n",
    "        )\n",
    "        stdout, stderr = process.communicate(input=sequence)\n",
    "        \n",
    "        if stderr and \"error\" in stderr.lower():\n",
    "            print(f\"RNAfold error: {stderr}\")\n",
    "            return None\n",
    "\n",
    "        lines = stdout.strip().split('\\n')\n",
    "        if len(lines) >= 2:\n",
    "            structure_line = lines[1]\n",
    "            dot_bracket = structure_line.split()[0]\n",
    "            return dot_bracket\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Execution Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    input_file = 'data/TRIM_Ded1_Dataset_Enhanced.csv'\n",
    "    output_file = '3_3_Plot/step1_full_structures_genomewide.csv'\n",
    "\n",
    "    # input_file = 'data/test_input.txt'\n",
    "    # output_file = '3_3_Plot/step1_full_structures.csv'\n",
    "    \n",
    "    print(f\"Loading {input_file}...\")\n",
    "    df = pd.read_csv(input_file)\n",
    "    \n",
    "    # 1. Sequence Concatenate\n",
    "    df['full_sequence'] = df['utr5_sequence'].astype(str) + df['cds_sequence'].astype(str)\n",
    "    structures = []\n",
    "    \n",
    "    print(\"Predicting full-length structure...\")\n",
    "    for seq in tqdm(df['full_sequence']):\n",
    "        if 'nan' in seq.lower():\n",
    "            structures.append(None)\n",
    "            continue\n",
    "            \n",
    "        struct = predict_structure(seq)\n",
    "        structures.append(struct)\n",
    "        \n",
    "    df['structure'] = structures\n",
    "    df_clean = df.dropna(subset=['structure'])\n",
    "    df_clean['utr_len'] = df_clean['utr5_sequence'].astype(str).apply(len)\n",
    "    df_clean.to_csv(output_file, index=False)\n",
    "    print(f\"Results saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046931cb",
   "metadata": {},
   "source": [
    "# Calculating Gradient Points\n",
    "**Switch to plot33 mamba env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b067b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import types\n",
    "\n",
    "class OmniMock:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.__spec__ = types.SimpleNamespace(origin=\"mock\")\n",
    "        self.__path__ = []\n",
    "        self.__version__ = \"9.9.9\"\n",
    "        self.__file__ = \"mock_file.py\"\n",
    "    \n",
    "    def __getattr__(self, name):\n",
    "        return OmniMock()\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return OmniMock()\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        return OmniMock()\n",
    "    \n",
    "    def __contains__(self, key):\n",
    "        return False\n",
    "mock_obj = OmniMock()\n",
    "BLOCK_LIST = [\n",
    "    \"torchvision\",\n",
    "    \"torchvision.ops\",\n",
    "    \"torchvision.transforms\",\n",
    "    \"torchvision.models\",\n",
    "    \"matplotlib\", \"matplotlib.pyplot\", \"matplotlib.colors\", \n",
    "    \"matplotlib.cm\", \"matplotlib.collections\", \"matplotlib.figure\",\n",
    "    \"matplotlib.image\", \"matplotlib.axes\", \"mpl_toolkits\", \n",
    "    \"mpl_toolkits.axes_grid1\",\n",
    "    \"scipy\", \"scipy.stats\", \"scipy.signal\", \"scipy.interpolate\",\n",
    "    \"scipy.spatial\", \"scipy.optimize\", \"scipy.sparse\", \"scipy.linalg\",\n",
    "    \"sklearn\", \"sklearn.preprocessing\", \"sklearn.utils\", \"sklearn.base\"\n",
    "]\n",
    "for lib_name in BLOCK_LIST:\n",
    "    sys.modules[lib_name] = mock_obj\n",
    "\n",
    "# =================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from captum.attr import IntegratedGradients\n",
    "from src.model_5U import Expert_5U\n",
    "\n",
    "NUC2IDX = {\"A\": 0, \"T\": 1, \"C\": 2, \"G\": 3, \"U\": 1}\n",
    "\n",
    "# ====================== Configuration ======================\n",
    "def get_cfg():\n",
    "    return {\n",
    "        \"in_csv_path\": \"3_3_Plot/step1_full_structures_genomewide.csv\",\n",
    "        \"ckpt_path\": \"outputs_5U/logs/version_0/checkpoints/local200-epoch=160-val/r2_reg=0.785.ckpt\",\n",
    "        \"out_csv_path\": \"3_3_Plot/step2_gradient_scores_genomewide.csv\",\n",
    "        \"L\": 200,\n",
    "        \"utr_tail_len\": 150,\n",
    "        \"cds_head_len\": 50,\n",
    "    }\n",
    "\n",
    "# ====================== Encoding Function ======================\n",
    "\n",
    "def encode_utr_cds_to_x200_with_text(utr_seq: str, cds_seq: str, L: int, utr_tail_len: int, cds_head_len: int):\n",
    "    utr = (str(utr_seq) if pd.notna(utr_seq) else \"\").upper().replace(\"U\", \"T\")\n",
    "    cds = (str(cds_seq) if pd.notna(cds_seq) else \"\").upper().replace(\"U\", \"T\")\n",
    "\n",
    "    if len(utr) >= utr_tail_len:\n",
    "        utr_tail = utr[-utr_tail_len:]\n",
    "    else:\n",
    "        utr_tail = utr\n",
    "\n",
    "    cds_head = cds[:cds_head_len]\n",
    "    merged = utr_tail + cds_head\n",
    "    merged = merged[-L:] \n",
    "    \n",
    "    x = np.zeros((4, L), dtype=np.float32)\n",
    "    start_pos = L - len(merged)\n",
    "\n",
    "    for i, base in enumerate(merged):\n",
    "        pos = start_pos + i\n",
    "        idx = NUC2IDX.get(base, None)\n",
    "        if idx is not None and 0 <= pos < L:\n",
    "            x[idx, pos] = 1.0\n",
    "\n",
    "    return x, merged, start_pos\n",
    "\n",
    "# ====================== Load Model ======================\n",
    "\n",
    "def load_expert5u(ckpt_path: str, device: torch.device) -> Expert_5U:\n",
    "    if not os.path.isfile(ckpt_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found: {ckpt_path}\")\n",
    "    \n",
    "    print(f\"[INFO] Loading checkpoint: {ckpt_path}\")\n",
    "    model = Expert_5U.load_from_checkpoint(ckpt_path, map_location=device)\n",
    "    model.to(device)\n",
    "    model.eval() \n",
    "    return model\n",
    "\n",
    "# ====================== Gradient Calculation ======================\n",
    "\n",
    "def run_gradient_analysis():\n",
    "    cfg = get_cfg()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[INFO] Device Used: {device}\")\n",
    "\n",
    "    if not os.path.exists(cfg[\"in_csv_path\"]):\n",
    "        raise FileNotFoundError(f\"Input not found: {cfg['in_csv_path']}\")\n",
    "        \n",
    "    df = pd.read_csv(cfg[\"in_csv_path\"])\n",
    "    model = load_expert5u(cfg[\"ckpt_path\"], device)\n",
    "    ig = IntegratedGradients(model)\n",
    "\n",
    "    L = cfg[\"L\"]\n",
    "    utr_tail_len = cfg[\"utr_tail_len\"]\n",
    "    cds_head_len = cfg[\"cds_head_len\"]\n",
    "    res_scores = []\n",
    "    res_seqs = []\n",
    "    res_start_pos = []\n",
    "\n",
    "    print(\"[INFO] Calculating Saliency Maps...\")\n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        tx_id = row['tx_id']\n",
    "        utr = row['utr5_sequence']\n",
    "        cds = row['cds_sequence']\n",
    "        x_numpy, effective_seq, start_pos = encode_utr_cds_to_x200_with_text(\n",
    "            utr, cds, L, utr_tail_len, cds_head_len\n",
    "        )\n",
    "        \n",
    "        input_tensor = torch.from_numpy(x_numpy).unsqueeze(0).to(device)\n",
    "        input_tensor.requires_grad = True \n",
    "        \n",
    "        try:\n",
    "            # Gradient Calculating\n",
    "            attributions = ig.attribute(input_tensor, target=None, n_steps=50, internal_batch_size=1)\n",
    "            \n",
    "            # Fusion\n",
    "            attr_score_tensor = torch.sum(torch.abs(attributions), dim=1)\n",
    "            attr_score = attr_score_tensor.squeeze(0)\n",
    "            \n",
    "            # Normalization\n",
    "            score_np = attr_score.detach().cpu().numpy()\n",
    "            denom = score_np.max() - score_np.min()\n",
    "            if denom == 0:\n",
    "                score_norm = np.zeros_like(score_np)\n",
    "            else:\n",
    "                score_norm = (score_np - score_np.min()) / denom\n",
    "            \n",
    "            res_scores.append(score_norm.tolist())\n",
    "            res_seqs.append(effective_seq)\n",
    "            res_start_pos.append(start_pos)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {tx_id}: {e}\")\n",
    "            res_scores.append(None)\n",
    "            res_seqs.append(None)\n",
    "            res_start_pos.append(None)\n",
    "\n",
    "    df['gradient_scores'] = res_scores\n",
    "    df['effective_sequence'] = res_seqs\n",
    "    df['start_pos'] = res_start_pos\n",
    "    df_clean = df.dropna(subset=['gradient_scores'])\n",
    "    os.makedirs(os.path.dirname(cfg[\"out_csv_path\"]), exist_ok=True)\n",
    "    df_clean.to_csv(cfg[\"out_csv_path\"], index=False)\n",
    "    \n",
    "    print(f\"[INFO] Gradient calculated. Files saved to {cfg['out_csv_path']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_gradient_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3df96a",
   "metadata": {},
   "source": [
    "# Give out final plot\n",
    "**Caution: Switch to draw mamba env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ast\n",
    "\n",
    "def analyze_trim_gradient_overlap(file_path, top_percent=10):\n",
    "    \"\"\"\n",
    "    Analyze the coverage of TRIM gradient with MAX30 PARS region.\n",
    "    \n",
    "    PARAMS:\n",
    "    - file_path: CSV Path\n",
    "    - top_percent: Threshold of high gradient, e.g. 5 refers Top 5%ã€‚\n",
    "    \"\"\"\n",
    "    print(f\"Loading files {file_path}, for top {top_percent}% gradient points.\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Loading failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Data Preprocessing ---\n",
    "    df_clean = df.dropna(subset=['Max30Pos']).copy()\n",
    "    print(f\"Valid lines with Max30Pos: {len(df_clean)} / {len(df)}\")\n",
    "\n",
    "    # Parsing lists\n",
    "    df_clean['gradient_scores'] = df_clean['gradient_scores'].apply(\n",
    "        lambda x: ast.literal_eval(x) if isinstance(x, str) else x\n",
    "    )\n",
    "\n",
    "    CDS_LEN = 50\n",
    "    aligned_gradients = []\n",
    "    hit_counts = 0\n",
    "    total_top_points = 0\n",
    "    region_grads_accum = []\n",
    "    global_grads_accum = []\n",
    "    \n",
    "    for index, row in df_clean.iterrows():\n",
    "        try:\n",
    "            full_grads = np.array(row['gradient_scores'])\n",
    "            utr_len = len(row['utr5_sequence'])\n",
    "            seq_len = utr_len + CDS_LEN\n",
    "            \n",
    "            # Cut for valid length\n",
    "            if len(full_grads) >= seq_len:\n",
    "                effective_grads = full_grads[-seq_len:]\n",
    "            else:\n",
    "                effective_grads = full_grads\n",
    "                \n",
    "            # Normalization to (0-1)\n",
    "            g_min, g_max = np.min(effective_grads), np.max(effective_grads)\n",
    "            if g_max - g_min == 0:\n",
    "                norm_grads = effective_grads - g_min\n",
    "            else:\n",
    "                norm_grads = (effective_grads - g_min) / (g_max - g_min)\n",
    "\n",
    "            # Position Max30\n",
    "            max30_rel_pos = int(row['Max30Pos'])\n",
    "            start_idx = utr_len + max30_rel_pos\n",
    "            end_idx = start_idx + 30\n",
    "            \n",
    "            # Boundary Check\n",
    "            start_idx = max(0, start_idx)\n",
    "            end_idx = min(len(effective_grads), end_idx)\n",
    "            \n",
    "            if start_idx >= end_idx:\n",
    "                continue\n",
    "\n",
    "            # Summarizing Top-K\n",
    "            top_k = max(1, int(len(effective_grads) * (top_percent / 100.0)))\n",
    "            top_indices = np.argsort(effective_grads)[-top_k:]\n",
    "            \n",
    "            hits = np.sum((top_indices >= start_idx) & (top_indices < end_idx))\n",
    "            hit_counts += hits\n",
    "            total_top_points += top_k\n",
    "            \n",
    "            region_grads_accum.extend(norm_grads[start_idx:end_idx])\n",
    "            global_grads_accum.extend(norm_grads)\n",
    "\n",
    "            # Preparing plot data (from -50 to +80 nt)\n",
    "            window_left = 50\n",
    "            window_right = 80\n",
    "            extract_start = start_idx - window_left\n",
    "            extract_end = start_idx + window_right\n",
    "            \n",
    "            aligned_window = np.full(window_left + window_right, np.nan)\n",
    "            \n",
    "            src_s = max(0, extract_start)\n",
    "            src_e = min(len(effective_grads), extract_end)\n",
    "            dst_s = src_s - extract_start\n",
    "            dst_e = dst_s + (src_e - src_s)\n",
    "            \n",
    "            if src_e > src_s:\n",
    "                aligned_window[dst_s:dst_e] = norm_grads[src_s:src_e]\n",
    "                aligned_gradients.append(aligned_window)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # --- Calculating Statistic Index ---\n",
    "    avg_hit_rate = hit_counts / total_top_points if total_top_points > 0 else 0\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Analysising finished\")\n",
    "    print(f\"Probabilities of top {top_percent}% gradients falling into Max30 regions: {avg_hit_rate:.2%}\")\n",
    "\n",
    "    # --- Plot ---\n",
    "    grad_matrix = np.array(aligned_gradients)\n",
    "    mean_profile = np.nanmean(grad_matrix, axis=0)\n",
    "    std_profile = (np.nanstd(grad_matrix, axis=0))**2\n",
    "    fig, ax = plt.subplots(figsize=(12, 5), dpi=300, facecolor='white')\n",
    "    ax.set_facecolor('white')\n",
    "    x_axis = np.arange(-50, 80)\n",
    "\n",
    "    ax.fill_between(x_axis, \n",
    "                    mean_profile - std_profile, \n",
    "                    mean_profile + std_profile, \n",
    "                    color='#0072B2', \n",
    "                    alpha=0.2, \n",
    "                    label='Variance')\n",
    "    \n",
    "    # Main Curve (Deep Blue)\n",
    "    ax.plot(x_axis, mean_profile, color='#0072B2', linewidth=3, label='Mean Gradient')\n",
    "    \n",
    "    # Max30 Region\n",
    "    ax.axvspan(0, 30, color='#D55E00', alpha=0.15, label='Max30 Region')\n",
    "    ax.axvline(0, color='black', linestyle='--', linewidth=1.5, alpha=0.5)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_color('black')\n",
    "    ax.spines['bottom'].set_linewidth(1.2)\n",
    "    ax.spines['left'].set_color('black')\n",
    "    ax.spines['left'].set_linewidth(1.2)\n",
    "    \n",
    "    ax.tick_params(axis='x', colors='black', labelsize=18, width=1.2)\n",
    "    ax.tick_params(axis='y', colors='black', labelsize=18, width=1.2)\n",
    "    ax.set_xlabel('Position relative to Max30 (nt)', fontweight=\"bold\", fontsize=24, color='black', labelpad=10)\n",
    "    ax.set_ylabel('Gradient Score', fontweight=\"bold\", fontsize=24, color='black', labelpad=10)\n",
    "    ax.legend(frameon=False, loc='upper right', fontsize=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "analyze_trim_gradient_overlap('3_3_Plot/step2_gradient_scores_genomewide.csv', top_percent=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "plot33",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
